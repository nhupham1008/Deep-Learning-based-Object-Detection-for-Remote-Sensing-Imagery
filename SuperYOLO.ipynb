{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1_XMUF5RwTEzWoOTRMg1LoBooTEpHptE6",
      "authorship_tag": "ABX9TyPDunwSfug8+8jCXCoFCyLj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhupham1008/Deep-Learning-based-Object-Detection-for-Remote-Sensing-Imagery/blob/main/SuperYOLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGqJHzbsLpo8",
        "outputId": "b0ab0181-cc3a-414a-f685-d1a0f527ca74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx9gFBPlWgKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d23344b-9b3f-4be2-b95c-bb43ae5c5561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SuperYOLO'...\n",
            "remote: Enumerating objects: 477, done.\u001b[K\n",
            "remote: Counting objects: 100% (160/160), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 477 (delta 101), reused 32 (delta 32), pack-reused 317\u001b[K\n",
            "Receiving objects: 100% (477/477), 16.94 MiB | 30.06 MiB/s, done.\n",
            "Resolving deltas: 100% (239/239), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/icey-zhang/SuperYOLO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/Object_Detection/SuperYOLO"
      ],
      "metadata": {
        "id": "V4yEEUOFgZOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZbBCO90-GgP",
        "outputId": "44da3986-d1c3-45c0-ea8d-4cf1694df44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.12)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG13AgtndHaK",
        "outputId": "412d1382-24a4-4e55-a307-06a207680849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.66.1)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.14.1)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (1.5.3)\n",
            "Requirement already satisfied: thop==0.0.31.post2005241907 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.0.31.post2005241907)\n",
            "Requirement already satisfied: pycocotools>=2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (2.0.7)\n",
            "Requirement already satisfied: xlsxwriter>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (3.1.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.1->-r requirements.txt (line 11)) (2.31.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.5.1)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 20)) (2023.3.post1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.1->-r requirements.txt (line 11)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.1->-r requirements.txt (line 11)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.1->-r requirements.txt (line 11)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.1->-r requirements.txt (line 11)) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 15)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "IQip6lrkQEtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/Object_Detection/SuperYOLO/dataset/"
      ],
      "metadata": {
        "id": "aAdzbOxfguev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# 1: car, 2:trucks, 4: tractors, 5: camping cars, 7: motorcycles, 8:buses, 9: vans, 10: others, 11: pickup, 23: boats , 201: Small Land Vehicles, 31: Large land Vehicles\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "PATH = '/content/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/' #chanhe the path firstly (PATH TO dataset)dddd\n",
        "\n",
        "def update_annotations(filename,image_size,label_path,save_path):\n",
        "    data = pd.read_csv(label_path + filename, sep=' ', index_col=None, header=None, names=['x_center', 'y_center', 'orientation', 'class', 'is_contained', 'is_occluded', 'corner1_x', 'corner2_x', 'corner3_x', 'corner4_x', 'corner1_y', 'corner2_y', 'corner3_y', 'corner4_y'])\n",
        "\n",
        "    data['class'].replace(1, 0, inplace=True)\n",
        "    data['class'].replace(11, 1, inplace=True)\n",
        "    data['class'].replace(2, 3, inplace=True)\n",
        "    data['class'].replace(5, 2, inplace=True)\n",
        "    data['class'].replace(4, 5, inplace=True)\n",
        "    data['class'].replace(10, 4, inplace=True)\n",
        "    data['class'].replace(23, 6, inplace=True)\n",
        "    data['class'].replace(9, 7, inplace=True)\n",
        "    data['x_center_ratio'] = data['x_center'].astype(float) / image_size\n",
        "    data['y_center_ratio'] = data['y_center'].astype(float) / image_size\n",
        "    data['width_ratio'] = (data[['corner1_x', 'corner2_x', 'corner3_x', 'corner4_x']].max(axis=1) - data[['corner1_x', 'corner2_x', 'corner3_x', 'corner4_x']].min(axis=1)) / image_size\n",
        "    data['height_ratio'] = (data[['corner1_y', 'corner2_y', 'corner3_y', 'corner4_y']].max(axis=1) - data[['corner1_y', 'corner2_y', 'corner3_y', 'corner4_y']].min(axis=1)) / image_size\n",
        "    res = data.drop(['x_center', 'y_center', 'corner1_x', 'corner2_x', 'corner3_x', 'corner4_x', 'orientation', 'corner1_y', 'corner2_y', 'corner3_y', 'corner4_y', 'is_contained', 'is_occluded'], axis=1)\n",
        "    res = res.drop(index=res.loc[(res['class'] >7)].index)\n",
        "    res.to_csv(save_path+ filename, sep=' ', index=False, header=None)\n",
        "\n",
        "def makelabels():\n",
        "    label_path = PATH + 'VEDAI/Annotations512'\n",
        "    save_path = PATH + 'VEDAI/labels'\n",
        "    list = os.listdir(label_path)\n",
        "    image_size = 512\n",
        "    for filename in list:\n",
        "        update_annotations(filename,image_size,label_path,save_path)\n",
        "\n",
        "\n",
        "def changepath():\n",
        "    for i in ['01','02','03','04','05','06','07','08','09','10']:\n",
        "        path = PATH + 'VEDAI/fold{}.txt'.format(i)\n",
        "        img_path = PATH + 'VEDAI_1024/images/'\n",
        "        write_path=(PATH + 'VEDAI/fold{}_write.txt').format(i)\n",
        "        with open(path, \"r\") as file:\n",
        "            img_files = file.readlines()\n",
        "            for j in range(len(img_files)):\n",
        "                img_files[j] =  img_path + img_files[j].rstrip()\n",
        "        file.close()\n",
        "        with open(write_path, \"w\") as file:\n",
        "            for j in range(len(img_files)):\n",
        "                file.write(img_files[j]+'\\n')\n",
        "        file.close()\n",
        "\n",
        "        path = PATH + 'VEDAI/fold{}test.txt'.format(i)\n",
        "        img_path = PATH + 'VEDAI/images/'\n",
        "        write_path=PATH + 'VEDAI/fold{}test_write.txt'.format(i)\n",
        "        with open(path, \"r\") as file:\n",
        "            img_files = file.readlines()\n",
        "            for j in range(len(img_files)):\n",
        "                img_files[j] =  img_path + img_files[j].rstrip()\n",
        "        file.close()\n",
        "        with open(write_path, \"w\") as file:\n",
        "            for j in range(len(img_files)):\n",
        "                file.write(img_files[j]+'\\n')\n",
        "        file.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    changepath()\n"
      ],
      "metadata": {
        "id": "_PKbI_K1hBrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/Object_Detection/SuperYOLO"
      ],
      "metadata": {
        "id": "gbZgZALh5tky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --cfg models/SRyolo_MF.yaml --super --train_img_size 1024 --hr_input --data data/SRvedai.yaml --ch 64 --input_mode RGB+IR+MF"
      ],
      "metadata": {
        "id": "w9sCBB8BRgRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd5c368-b884-471c-ba8f-e73c0e72b8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-18 15:41:39.178046: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-18 15:41:39.178107: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-18 15:41:39.178148: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-18 15:41:40.203448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "fatal: ambiguous argument 'main..origin/master': unknown revision or path not in the working tree.\n",
            "Use '--' to separate paths from revisions, like this:\n",
            "'git <command> [<revision>...] -- [<file>...]'\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mCommand 'git rev-list main..origin/master --count' returned non-zero exit status 128.\n",
            "YOLOv5 🚀 fcd54af torch 2.1.0+cu118 CUDA:0 (Tesla T4, 15102.0625MB)\n",
            "\n",
            "Namespace(weights='', cfg='models/SRyolo_MF.yaml', super=True, data='data/SRvedai.yaml', hyp='data/hyp.scratch.yaml', epochs=300, ch_steam=3, ch=64, input_mode='RGB+IR+MF', batch_size=2, train_img_size=1024, test_img_size=512, hr_input=True, rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=4, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', world_size=1, global_rank=-1, img_size=[1024, 512], save_dir='runs/train/exp7', total_batch_size=2)\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOv5 logging with 'pip install wandb' (recommended)\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1958  models.common.MF                        [3]                           \n",
            "  1                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18              [17]  1      5031  models.SRyolo.Detect                    [8, [[10, 13, 16, 30, 33, 23]], [128]]\n",
            "Reversing anchor order\n",
            "Model Summary: 323 layers, 7732753 parameters, 7732753 gradients,  GFLOPS\n",
            "\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI_1024/labels.cache' for images and labels... 1089 found, 0 missing, 8 empty, 12 corrupted: 100% 1089/1089 [00:00<?, ?it/s]\n",
            "[ WARN:0@6.258] global loadsave.cpp:248 findDecoder imread_('/irntent/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI_1024/images/00000000_ir.png'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@6.284] global loadsave.cpp:248 findDecoder imread_('/irntent/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI_1024/images/00000006_ir.png'): can't open/read file: check file path/integrity\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI/labels.cache' for images and labels... 121 found, 0 missing, 2 empty, 0 corrupted: 100% 121/121 [00:00<?, ?it/s]\n",
            "[ WARN:0@6.369] global loadsave.cpp:248 findDecoder imread_('/irntent/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI_1024/images/00000040_ir.png'): can't open/read file: check file path/integrity\n",
            "Plotting labels... \n",
            "[ WARN:0@6.406] global loadsave.cpp:248 findDecoder imread_('/irntent/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI_1024/images/00000070_ir.png'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@6.409] global loadsave.cpp:248 findDecoder imread_('/irntent/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI/images/00000010_ir.png'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@6.414] global loadsave.cpp:248 findDecoder imread_('/irntent/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI/images/00000025_ir.png'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@6.437] global loadsave.cpp:248 findDecoder imread_('/irntent/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI/images/00000087_ir.png'): can't open/read file: check file path/integrity\n",
            "[ WARN:0@6.438] global loadsave.cpp:248 findDecoder imread_('/irntent/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI/images/00000055_ir.png'): can't open/read file: check file path/integrity\n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.36, Best Possible Recall (BPR) = 0.9821\n",
            "Image sizes 1024 train, 512 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/exp7\n",
            "Starting training for 300 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "  0% 0/539 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/train.py\", line 673, in <module>\n",
            "    train(hyp, opt, device, tb_writer)\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/train.py\", line 343, in train\n",
            "    for i, (imgs, irs, targets, paths, _) in pbar:  # batch zjq  -------------------------------------------------------------\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1182, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/utils/datasets.py\", line 132, in __iter__\n",
            "    yield next(self.iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\n",
            "    return self._process_data(data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 694, in reraise\n",
            "    raise exception\n",
            "AssertionError: Caught AssertionError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
            "    data = fetcher.fetch(index)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/utils/datasets.py\", line 823, in __getitem__\n",
            "    img, ir, labels = load_mosaic(self, index) #zjq\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/utils/datasets.py\", line 1008, in load_mosaic\n",
            "    ir = load_ir(self, index) #zjq\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/utils/datasets.py\", line 968, in load_ir\n",
            "    assert ir is not None, 'Image_ir Not Found ' + path\n",
            "AssertionError: Image_ir Not Found /irntent/drive/MyDrive/Object_Detection/SuperYOLO/dataset/VEDAIdataset/VEDAI_1024/images/00000000_ir.png\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --cfg models/SRyolo_noFocus_small.yaml --super --train_img_size 1024 --hr_input --data data/SRvedai.yaml --ch 3 --input_mode RGB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QDKkwIf66ph",
        "outputId": "545af043-29a0-4171-fdd2-cb12e6f3d9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-18 14:14:15.593772: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-18 14:14:15.593831: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-18 14:14:15.593882: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-18 14:14:15.605936: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-18 14:14:17.066901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/train.py\", line 26, in <module>\n",
            "    import test\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/test.py\", line 12, in <module>\n",
            "    from models.experimental import attempt_load\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/experimental.py\", line 7, in <module>\n",
            "    from models.common import Conv, DWConv\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/common.py\", line 19, in <module>\n",
            "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
            "ModuleNotFoundError: No module named 'timm'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --cfg models/SRyolo_noFocus_small.yaml --super --train_img_size 1024 --hr_input --data data/SRvedai.yaml --ch 3 --input_mode IR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehhIIGUs667x",
        "outputId": "90ef7243-b8d0-478b-c2ba-60988decb892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-18 14:14:43.318578: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-18 14:14:43.318648: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-18 14:14:43.318694: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-18 14:14:43.330312: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-18 14:14:44.808150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/train.py\", line 26, in <module>\n",
            "    import test\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/test.py\", line 12, in <module>\n",
            "    from models.experimental import attempt_load\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/experimental.py\", line 7, in <module>\n",
            "    from models.common import Conv, DWConv\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/common.py\", line 19, in <module>\n",
            "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
            "ModuleNotFoundError: No module named 'timm'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --cfg models/SRyolo_MF.yaml --train_img_size 512 --data data/SRvedai.yaml --ch 64 --input_mode RGB+IR+MF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcMp_8YN67Fj",
        "outputId": "d0fd6b95-46c3-4d34-f839-36b442796943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-18 14:15:12.956122: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-18 14:15:12.956237: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-18 14:15:12.956322: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-18 14:15:12.964163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-18 14:15:14.054607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/train.py\", line 26, in <module>\n",
            "    import test\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/test.py\", line 12, in <module>\n",
            "    from models.experimental import attempt_load\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/experimental.py\", line 7, in <module>\n",
            "    from models.common import Conv, DWConv\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/common.py\", line 19, in <module>\n",
            "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
            "ModuleNotFoundError: No module named 'timm'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --cfg models/SRyolo_noFocus_small.yaml --train_img_size 512 --data data/SRvedai.yaml --ch 3 --input_mode RGB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3QAxbaR7Sxt",
        "outputId": "73a6abf9-2094-431e-d3fd-6a991643dc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-18 14:15:46.209114: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-18 14:15:46.209171: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-18 14:15:46.209210: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-18 14:15:46.217056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-18 14:15:47.377310: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/train.py\", line 26, in <module>\n",
            "    import test\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/test.py\", line 12, in <module>\n",
            "    from models.experimental import attempt_load\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/experimental.py\", line 7, in <module>\n",
            "    from models.common import Conv, DWConv\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/common.py\", line 19, in <module>\n",
            "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
            "ModuleNotFoundError: No module named 'timm'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --cfg models/SRyolo_noFocus_small.yaml --train_img_size 512 --data data/SRvedai.yaml --ch 3 --input_mode IR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFJQtTzX7S6f",
        "outputId": "0bcbb97c-a7b8-4aa7-b920-cad58bff613d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-18 14:16:08.056095: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-18 14:16:08.056155: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-18 14:16:08.056196: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-18 14:16:08.064292: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-18 14:16:09.093478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/train.py\", line 26, in <module>\n",
            "    import test\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/test.py\", line 12, in <module>\n",
            "    from models.experimental import attempt_load\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/experimental.py\", line 7, in <module>\n",
            "    from models.common import Conv, DWConv\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/common.py\", line 19, in <module>\n",
            "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
            "ModuleNotFoundError: No module named 'timm'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --weights runs/train/exp/best.pt --input_mode RGB+IR+MF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee0RTaQv7kpz",
        "outputId": "a2303d1a-15be-4e84-b7c1-f079c8033222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/test.py\", line 12, in <module>\n",
            "    from models.experimental import attempt_load\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/experimental.py\", line 7, in <module>\n",
            "    from models.common import Conv, DWConv\n",
            "  File \"/content/drive/MyDrive/Object_Detection/SuperYOLO/models/common.py\", line 19, in <module>\n",
            "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
            "ModuleNotFoundError: No module named 'timm'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from threading import Thread\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import create_dataloader, create_dataloader_sr\n",
        "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n",
        "    box_iou, non_max_suppression,weighted_boxes, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n",
        "from utils.metrics import ap_per_class, ConfusionMatrix\n",
        "from utils.plots import plot_images, output_to_target, plot_study_txt\n",
        "from utils.torch_utils import select_device, time_synchronized\n",
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "unloader = transforms.ToPILImage()\n",
        "def tensor_to_PIL(tensor):\n",
        "    image = tensor.cpu().clone()\n",
        "    image = image.squeeze(0)\n",
        "    image = unloader(image)\n",
        "    image.save('a.png')\n",
        "    return image\n",
        "\n",
        "def test(data,\n",
        "         weights=None,\n",
        "         batch_size=32,\n",
        "         imgsz=640,\n",
        "         input_mode = None,\n",
        "         conf_thres=0.001,\n",
        "         iou_thres=0.6,  # for NMS\n",
        "         save_json=False,\n",
        "         single_cls=False,\n",
        "         augment=False,\n",
        "         verbose=False,\n",
        "         model=None,\n",
        "         dataloader=None,\n",
        "         save_dir=Path(''),  # for saving images\n",
        "         save_txt=False,  # for auto-labelling\n",
        "         save_hybrid=False,  # for hybrid auto-labelling\n",
        "         save_conf=False,  # save auto-label confidences\n",
        "         plots=True,\n",
        "         wandb_logger=None,\n",
        "         compute_loss=None,\n",
        "         is_coco=False):\n",
        "    # Initialize/load model and set device\n",
        "    training = model is not None\n",
        "    if training:  # called by train.py\n",
        "        device = next(model.parameters()).device  # get model device\n",
        "\n",
        "    else:  # called directly\n",
        "        set_logging()\n",
        "        device = select_device(opt.device, batch_size=batch_size)\n",
        "\n",
        "        # Directories\n",
        "        save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
        "        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "        # Load model\n",
        "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "        #zjq\n",
        "        # print(model)\n",
        "        print(model.yaml_file)\n",
        "        # import thop\n",
        "        # input_image_size = opt.img_size\n",
        "        # input_image = torch.randn(1, 3, input_image_size, input_image_size).to(device)\n",
        "        # flops, params = thop.profile(model, inputs=(input_image,input_image,input_mode), verbose=False)\n",
        "        # print('Params: %.4fM'%(params/1e6))\n",
        "        # print('FLOPs: %.2fG'%(flops/1e9))\n",
        "        # model=torch.jit.trace(model,(input_image,input_image)).eval()\n",
        "        #print(model)\n",
        "        #print('Layers: %.0f'%(len(list(model.modules()))))\n",
        "        gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
        "        imgsz = check_img_size(imgsz, s=gs)  # check img_size\n",
        "\n",
        "        # Multi-GPU disabled, incompatible with .half() https://github.com/ultralytics/yolov5/issues/99\n",
        "        # if device.type != 'cpu' and torch.cuda.device_count() > 1:\n",
        "        #     model = nn.DataParallel(model)\n",
        "\n",
        "    # Half\n",
        "    # half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "    # if half:\n",
        "    #     model.half()\n",
        "\n",
        "    # Configure\n",
        "    model.eval()\n",
        "\n",
        "    # print(model)\n",
        "    if isinstance(data, str):\n",
        "        is_coco = data.endswith('coco.yaml')\n",
        "        with open(data) as f:\n",
        "            data = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "    check_dataset(data)  # check\n",
        "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
        "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95 zjq\n",
        "    niou = iouv.numel()\n",
        "\n",
        "    # Logging\n",
        "    log_imgs = 0\n",
        "    if wandb_logger and wandb_logger.wandb:\n",
        "        log_imgs = min(wandb_logger.log_imgs, 100)\n",
        "    # Dataloader\n",
        "    if not training:\n",
        "        # if device.type != 'cpu': #zjq zhushi\n",
        "        #     model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
        "        task = opt.task if opt.task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
        "        dataloader = create_dataloader_sr(data[task], imgsz, batch_size, gs, opt, pad=0.5, rect=True,\n",
        "                            prefix=colorstr(f'{task}: '))[0]\n",
        "\n",
        "    seen = 0\n",
        "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
        "    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
        "    coco91class = coco80_to_coco91_class()\n",
        "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
        "    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
        "    loss = torch.zeros(3, device=device)\n",
        "    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
        "    for batch_i, (img, ir, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)): #zjq\n",
        "        img = img.to(device, non_blocking=True).float()\n",
        "        # img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        ir = ir.to(device, non_blocking=True).float()\n",
        "        # ir = ir.half() if half else ir.float()  # uint8 to fp16/32\n",
        "        ir /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        targets = targets.to(device)\n",
        "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Run model\n",
        "            t = time_synchronized()\n",
        "            try:\n",
        "                out, train_out = model(img,ir,input_mode=input_mode) #zjq inference and training outputs\n",
        "            except:\n",
        "                out, train_out,_ = model(img,ir,input_mode=input_mode) #zjq inference and training outputs\n",
        "            t0 += time_synchronized() - t\n",
        "\n",
        "            # Compute loss\n",
        "            if compute_loss:\n",
        "                loss += compute_loss([x.float() for x in train_out], targets)[1][:3]  # box, obj, cls\n",
        "\n",
        "            # Run NMS\n",
        "            targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
        "            lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n",
        "            t = time_synchronized()\n",
        "            out = non_max_suppression(out, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)\n",
        "            # out = weighted_boxes(out,image_size=imgsz, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)\n",
        "            t1 += time_synchronized() - t\n",
        "\n",
        "        # Statistics per image\n",
        "        for si, pred in enumerate(out):\n",
        "            labels = targets[targets[:, 0] == si, 1:]\n",
        "            nl = len(labels)\n",
        "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
        "            path = Path(paths[si])\n",
        "            seen += 1\n",
        "\n",
        "            if len(pred) == 0:\n",
        "                if nl:\n",
        "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
        "                continue\n",
        "\n",
        "            # Predictions\n",
        "            predn = pred.clone()\n",
        "            scale_coords(img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])  # native-space pred\n",
        "\n",
        "            # Append to text file\n",
        "            if save_txt:\n",
        "                gn = torch.tensor(shapes[si][0])[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "                for *xyxy, conf, cls in predn.tolist():\n",
        "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                    with open(save_dir / 'labels' / (path.stem + '.txt'), 'a') as f:\n",
        "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "            # W&B logging - Media Panel Plots\n",
        "            if len(wandb_images) < log_imgs and wandb_logger.current_epoch > 0:  # Check for test operation\n",
        "                if wandb_logger.current_epoch % wandb_logger.bbox_interval == 0:\n",
        "                    box_data = [{\"position\": {\"minX\": xyxy[0], \"minY\": xyxy[1], \"maxX\": xyxy[2], \"maxY\": xyxy[3]},\n",
        "                                 \"class_id\": int(cls),\n",
        "                                 \"box_caption\": \"%s %.3f\" % (names[cls], conf),\n",
        "                                 \"scores\": {\"class_score\": conf},\n",
        "                                 \"domain\": \"pixel\"} for *xyxy, conf, cls in pred.tolist()]\n",
        "                    boxes = {\"predictions\": {\"box_data\": box_data, \"class_labels\": names}}  # inference-space\n",
        "                    wandb_images.append(wandb_logger.wandb.Image(img[si], boxes=boxes, caption=path.name))\n",
        "            wandb_logger.log_training_progress(predn, path, names) if wandb_logger and wandb_logger.wandb_run else None\n",
        "\n",
        "            # Append to pycocotools JSON dictionary\n",
        "            if save_json:\n",
        "                # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
        "                image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n",
        "                box = xyxy2xywh(predn[:, :4])  # xywh\n",
        "                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
        "                for p, b in zip(pred.tolist(), box.tolist()):\n",
        "                    jdict.append({'image_id': image_id,\n",
        "                                  'category_id': coco91class[int(p[5])] if is_coco else int(p[5]),\n",
        "                                  'bbox': [round(x, 3) for x in b],\n",
        "                                  'score': round(p[4], 5)})\n",
        "\n",
        "            # Assign all predictions as incorrect\n",
        "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
        "            if nl:\n",
        "                detected = []  # target indices\n",
        "                tcls_tensor = labels[:, 0]\n",
        "\n",
        "                # target boxes\n",
        "                tbox = xywh2xyxy(labels[:, 1:5])\n",
        "                scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n",
        "                if plots:\n",
        "                    confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))\n",
        "\n",
        "                # Per target class\n",
        "                for cls in torch.unique(tcls_tensor):\n",
        "                    ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
        "                    pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
        "\n",
        "                    # Search for detections\n",
        "                    if pi.shape[0]:\n",
        "                        # Prediction to target ious\n",
        "                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
        "\n",
        "                        # Append detections\n",
        "                        detected_set = set()\n",
        "                        for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
        "                            d = ti[i[j]]  # detected target\n",
        "                            if d.item() not in detected_set:\n",
        "                                detected_set.add(d.item())\n",
        "                                detected.append(d)\n",
        "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
        "                                if len(detected) == nl:  # all targets already located in image\n",
        "                                    break\n",
        "\n",
        "            # Append statistics (correct, conf, pcls, tcls)\n",
        "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
        "\n",
        "        # Plot images\n",
        "        if plots: #and batch_i < 3: #zjq\n",
        "            f = save_dir / f'test_batch{batch_i}_labels.png'  # labels\n",
        "            # f = '/home/data/zhangjiaqing/dataset/VEDAI/train_label/'+paths[0].split('/')[-1].replace('_co','_label') #zjq\n",
        "            if input_mode == 'IR':\n",
        "                Thread(target=plot_images, args=(ir, targets, paths, f, names), daemon=True).start()\n",
        "            else:\n",
        "                Thread(target=plot_images, args=(img, targets, paths, f, names), daemon=True).start()\n",
        "            f = save_dir / f'test_batch{batch_i}_pred.png'  # predictions\n",
        "            if input_mode == 'IR':\n",
        "                Thread(target=plot_images, args=(ir, output_to_target(out), paths, f, names), daemon=True).start()\n",
        "            else:\n",
        "                Thread(target=plot_images, args=(img, output_to_target(out), paths, f, names), daemon=True).start()\n",
        "    # Compute statistics\n",
        "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
        "    if len(stats) and stats[0].any():\n",
        "        p, r, ap, f1, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n",
        "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
        "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
        "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
        "    else:\n",
        "        nt = torch.zeros(1)\n",
        "\n",
        "    # Print results\n",
        "    pf = '%20s' + '%12i' * 2 + '%12.4g' * 4  # print format\n",
        "    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
        "    # with open(\"trying.txt\", 'a+') as f:\n",
        "    #     f.write((pf % ('all', seen, nt.sum(), mp, mr, map50, map)) + '\\n')  # append metrics, val_loss\n",
        "\n",
        "    import xlsxwriter\n",
        "    workbook = xlsxwriter.Workbook('hello.xlsx') # 建立文件\n",
        "    worksheet = workbook.add_worksheet() # 建立sheet， 可以work.add_worksheet('employee')来指定sheet名，但中文名会报UnicodeDecodeErro的错误\n",
        "    worksheet.write(0,0, 'all') # 向A1写入\n",
        "    worksheet.write(0,1, seen) # 向A1写入\n",
        "    worksheet.write(0,2,nt.sum())#向第二行第二例写入guoshun\n",
        "    worksheet.write(0,3,mp*100)\n",
        "    worksheet.write(0,4,mr*100)\n",
        "    worksheet.write(0,5,map50*100)\n",
        "    worksheet.write(0,6,map*100)\n",
        "\n",
        "\n",
        "\n",
        "    # Print results per class\n",
        "    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
        "        for i, c in enumerate(ap_class):\n",
        "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
        "            # with open(\"trying.txt\", 'a+') as f:\n",
        "            #     f.write((pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i])) + '\\n')  # append metrics, val_loss\n",
        "            worksheet.write(i+1,0, names[c]) # 向A1写入\n",
        "            worksheet.write(i+1,1, seen) # 向A1写入\n",
        "            worksheet.write(i+1,2,nt[c])#向第二行第二例写入\n",
        "            worksheet.write(i+1,3,p[i]*100)\n",
        "            worksheet.write(i+1,4,r[i]*100)\n",
        "            worksheet.write(i+1,5,ap50[i]*100)\n",
        "            worksheet.write(i+1,6,ap[i]*100)\n",
        "        workbook.close()\n",
        "        with open(\"trying.txt\", 'a+') as f:\n",
        "            f.write('\\n')  # append metrics, val_loss\n",
        "    # Print speeds\n",
        "    t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n",
        "    if not training:\n",
        "        print('Speed: %.3f/%.3f/%.3f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
        "    # Plots\n",
        "    if plots:\n",
        "        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
        "        if wandb_logger and wandb_logger.wandb:\n",
        "            val_batches = [wandb_logger.wandb.Image(str(f), caption=f.name) for f in sorted(save_dir.glob('test*.jpg'))]\n",
        "            wandb_logger.log({\"Validation\": val_batches})\n",
        "    if wandb_images:\n",
        "        wandb_logger.log({\"Bounding Box Debugger/Images\": wandb_images})\n",
        "\n",
        "    # Save JSON\n",
        "    if save_json and len(jdict):\n",
        "        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''  # weights\n",
        "        anno_json = '../coco/annotations/instances_val2017.json'  # annotations json\n",
        "        pred_json = str(save_dir / f\"{w}_predictions.json\")  # predictions json\n",
        "        print('\\nEvaluating pycocotools mAP... saving %s...' % pred_json)\n",
        "        with open(pred_json, 'w') as f:\n",
        "            json.dump(jdict, f)\n",
        "\n",
        "        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n",
        "            from pycocotools.coco import COCO\n",
        "            from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "            anno = COCO(anno_json)  # init annotations api\n",
        "            pred = anno.loadRes(pred_json)  # init predictions api\n",
        "            eval = COCOeval(anno, pred, 'bbox')\n",
        "            if is_coco:\n",
        "                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]  # image IDs to evaluate\n",
        "            eval.evaluate()\n",
        "            eval.accumulate()\n",
        "            eval.summarize()\n",
        "            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
        "        except Exception as e:\n",
        "            print(f'pycocotools unable to run: {e}')\n",
        "\n",
        "    # Return results\n",
        "    model.float()  # for training\n",
        "    if not training:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        print(f\"Results saved to {save_dir}{s}\")\n",
        "    maps = np.zeros(nc) + map\n",
        "    for i, c in enumerate(ap_class):\n",
        "        maps[c] = ap[i]\n",
        "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(prog='test.py')\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default='small_EDSR_fold1.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--data', type=str, default='data/SRvedai.yaml', help='*.data path')\n",
        "    parser.add_argument('--batch-size', type=int, default=1, help='size of each image batch')\n",
        "    parser.add_argument('--img-size', type=int, default=512, help='inference size (pixels)')\n",
        "    parser.add_argument('--input_mode', type=str, default='RGB+IR') #RGB IR RGB+IR RGB+IR+fusion\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.001, help='object confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.6, help='IOU threshold for NMS')\n",
        "    parser.add_argument('--task', default='val', help='train, val, test, speed or study')\n",
        "    parser.add_argument('--device', default='1', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--verbose', action='store_true', help='report mAP by class')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')\n",
        "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "    parser.add_argument('--save-json', action='store_true', help='save a cocoapi-compatible JSON results file')\n",
        "    parser.add_argument('--project', default='runs/test', help='save to project/name')\n",
        "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    opt = parser.parse_args()\n",
        "    opt.save_json |= opt.data.endswith('coco.yaml')\n",
        "    opt.data = check_file(opt.data)  # check file\n",
        "    print(opt)\n",
        "    check_requirements()\n",
        "    if opt.task in ('train', 'val', 'test'):  # run normally\n",
        "        test(opt.data,\n",
        "            opt.weights,\n",
        "            opt.batch_size,\n",
        "            opt.img_size,\n",
        "            opt.input_mode,\n",
        "            opt.conf_thres,\n",
        "            opt.iou_thres,\n",
        "            opt.save_json,\n",
        "            opt.single_cls,\n",
        "            opt.augment,\n",
        "            opt.verbose,\n",
        "            save_txt=opt.save_txt | opt.save_hybrid,\n",
        "            save_hybrid=opt.save_hybrid,\n",
        "            save_conf=opt.save_conf,\n",
        "            )\n",
        "\n",
        "    elif opt.task == 'speed':  # speed benchmarks\n",
        "        for w in opt.weights:\n",
        "            test(opt.data, w, opt.batch_size, opt.img_size, 0.25, 0.45, save_json=False, plots=False)\n",
        "\n",
        "    elif opt.task == 'study':  # run over a range of settings and save/plot\n",
        "        # python test.py --task study --data coco.yaml --iou 0.7 --weights yolov5s.pt yolov5m.pt yolov5l.pt yolov5x.pt\n",
        "        x = list(range(256, 1536 + 128, 128))  # x axis (image sizes)\n",
        "        for w in opt.weights:\n",
        "            f = f'study_{Path(opt.data).stem}_{Path(w).stem}.txt'  # filename to save to\n",
        "            y = []  # y axis\n",
        "            for i in x:  # img-size\n",
        "                print(f'\\nRunning {f} point {i}...')\n",
        "                r, _, t = test(opt.data, w, opt.batch_size, i, opt.conf_thres, opt.iou_thres, opt.save_json,\n",
        "                                plots=False)\n",
        "                y.append(r + t)  # results and times\n",
        "            np.savetxt(f, y, fmt='%10.4g')  # save\n",
        "        os.system('zip -r study.zip study_*.txt')\n",
        "        plot_study_txt(x=x)  # plot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "oppB5iFt8wi-",
        "outputId": "6b394850-1249-41c5-dce8-07a0fde4b051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'timm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-eb7d4596fc9f>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_dataloader_sr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoco80_to_coco91_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_img_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_requirements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Object_Detection/SuperYOLO/models/experimental.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDWConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Object_Detection/SuperYOLO/models/common.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime_synchronized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_2tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSiLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# export-friendly version of nn.SiLU()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'timm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}